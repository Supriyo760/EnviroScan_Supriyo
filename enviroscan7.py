# Enviroscan7 Streamlit App
# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
import osmnx as ox
import requests
from datetime import datetime, timezone
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import cross_validate
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import KFold
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
# --- Constants ---
POLLUTANTS = ["pm25", "pm10", "no2", "co", "so2", "o3"]
OPENWEATHER_KEY = "f931ecc3a4864ae98a35630e7a9f5bc2"
# --- Helper Functions ---
def get_weather(lat, lon, api_key):
Â Â Â Â url = "http://api.openweathermap.org/data/2.5/weather"
Â Â Â Â resp = requests.get(url, params={"lat": lat, "lon": lon, "appid": api_key, "units": "metric"})
Â Â Â Â return resp.json() if resp.status_code == 200 else {}
def extract_osm_features(lat, lon, radius=2000): # Increased radius for better feature capture
Â Â Â Â features = {}
Â Â Â Â point = (lat, lon)
Â Â Â Â try:
Â Â Â Â Â Â Â Â roads = ox.features_from_point(point, tags={"highway": True}, dist=radius)
Â Â Â Â Â Â Â Â features["roads_count"] = len(roads)
Â Â Â Â except:
Â Â Â Â Â Â Â Â features["roads_count"] = 0
Â Â Â Â try:
Â Â Â Â Â Â Â Â industries = ox.features_from_point(point, tags={"landuse": ["industrial", "commercial"]}, dist=radius)
Â Â Â Â Â Â Â Â features["industries_count"] = len(industries)
Â Â Â Â except:
Â Â Â Â Â Â Â Â features["industries_count"] = 0
Â Â Â Â try:
Â Â Â Â Â Â Â Â farms = ox.features_from_point(point, tags={"landuse": ["farmland", "farm", "agricultural"]}, dist=radius)
Â Â Â Â Â Â Â Â features["farms_count"] = len(farms)
Â Â Â Â except:
Â Â Â Â Â Â Â Â features["farms_count"] = 0
Â Â Â Â try:
Â Â Â Â Â Â Â Â dumps = ox.features_from_point(point, tags={"landuse": ["landfill", "waste", "dump"]}, dist=radius)
Â Â Â Â Â Â Â Â features["dumps_count"] = len(dumps)
Â Â Â Â except:
Â Â Â Â Â Â Â Â features["dumps_count"] = 0
Â Â Â Â return features
# --- Cleanup Script ---
with open("enviroscan7.py", "r", encoding="utf-8") as f:
    code = f.read()

# Replace invisible U+00A0 with normal space
code = code.replace("\u00a0", " ")

with open("enviroscan7.py", "w", encoding="utf-8") as f:
    f.write(code)

print("âœ… Cleaned non-breaking spaces")

def build_dataset(city, lat, lon, aq_csv_file, openweather_key):
Â Â Â Â try:
Â Â Â Â Â Â Â Â df_aq = pd.read_csv(
Â Â Â Â Â Â Â Â Â Â Â Â aq_csv_file,
Â Â Â Â Â Â Â Â Â Â Â Â skiprows=2, # Adjust if your CSV has different header rows
Â Â Â Â Â Â Â Â Â Â Â Â on_bad_lines="skip",
Â Â Â Â Â Â Â Â Â Â Â Â engine="python"
Â Â Â Â Â Â Â Â )
Â Â Â Â Â Â Â Â df_aq = df_aq.loc[:, ~df_aq.columns.str.contains("^Unnamed")]
Â Â Â Â Â Â Â Â df_aq["source"] = "OpenAQ"
Â Â Â Â Â Â Â 
Â Â Â Â Â Â Â Â # Debug: Verify stations (optional, remove after testing)
Â Â Â Â Â Â Â Â # st.write("Unique stations in raw CSV:", df_aq['location_name'].nunique())
Â Â Â Â Â Â Â Â # st.write("Stations:", df_aq['location_name'].unique().tolist())
Â Â Â Â Â Â Â 
Â Â Â Â except Exception as e:
Â Â Â Â Â Â Â Â st.error(f"âš ï¸ Failed to load AQ CSV: {e}")
Â Â Â Â Â Â Â Â return pd.DataFrame(), {}
Â Â Â 
Â Â Â Â # Ensure latitude and longitude are present
Â Â Â Â if 'latitude' not in df_aq.columns or 'longitude' not in df_aq.columns:
Â Â Â Â Â Â Â Â df_aq['latitude'] = lat
Â Â Â Â Â Â Â Â df_aq['longitude'] = lon
Â Â Â 
Â Â Â Â # Define expected pollutants
Â Â Â Â POLLUTANTS = ['pm25', 'pm10', 'no2', 'co', 'so2', 'o3']
Â Â Â 
Â Â Â Â # Convert CO from ppb to Âµg/mÂ³ for consistency
Â Â Â Â df_aq.loc[df_aq['parameter'] == 'co', 'value'] *= 1144.6 # ppb to Âµg/mÂ³ (approx, at 25Â°C, 1 atm)
Â Â Â 
Â Â Â Â # Pivot to wide format per location/timestamp
Â Â Â Â df_agg = df_aq.groupby(['location_name', 'latitude', 'longitude', 'datetimeUtc', 'parameter'])['value'].mean().reset_index()
Â Â Â Â df_wide = df_agg.pivot_table(
Â Â Â Â Â Â Â Â index=['location_name', 'latitude', 'longitude', 'datetimeUtc'],
Â Â Â Â Â Â Â Â columns='parameter',
Â Â Â Â Â Â Â Â values='value',
Â Â Â Â Â Â Â Â aggfunc='mean' # Handle duplicates
Â Â Â Â ).reset_index()
Â Â Â 
Â Â Â Â # Initialize missing pollutant columns
Â Â Â Â for pollutant in POLLUTANTS:
Â Â Â Â Â Â Â Â if pollutant not in df_wide.columns:
Â Â Â Â Â Â Â Â Â Â Â Â df_wide[pollutant] = np.nan
Â Â Â 
Â Â Â Â # Fill missing values PER STATION
Â Â Â Â df_wide = df_wide.sort_values(['location_name', 'datetimeUtc'])
Â Â Â Â df_wide[POLLUTANTS] = df_wide.groupby('location_name')[POLLUTANTS].fillna(method='ffill').fillna(method='bfill')
Â Â Â 
Â Â Â Â # Add OSM features per unique location
Â Â Â Â unique_locations = df_wide[['location_name', 'latitude', 'longitude']].drop_duplicates()
Â Â Â Â osm_dict = {}
Â Â Â Â for _, row in unique_locations.iterrows():
Â Â Â Â Â Â Â Â osm = extract_osm_features(row['latitude'], row['longitude'], radius=2000)
Â Â Â Â Â Â Â Â key = (row['location_name'], row['latitude'], row['longitude'])
Â Â Â Â Â Â Â Â osm_dict[key] = osm
Â Â Â 
Â Â Â Â df_osm = df_wide.apply(lambda row: pd.Series(osm_dict.get((row['location_name'], row['latitude'], row['longitude']),
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â {'roads_count': 0, 'industries_count': 0, 'farms_count': 0, 'dumps_count': 0})), axis=1)
Â Â Â Â df_wide = pd.concat([df_wide, df_osm], axis=1)
Â Â Â 
Â Â Â Â # Add weather data
Â Â Â Â weather_data = get_weather(lat, lon, openweather_key)
Â Â Â Â weather_features = {
Â Â Â Â Â Â Â Â 'temp_c': weather_data.get('main', {}).get('temp'),
Â Â Â Â Â Â Â Â 'humidity': weather_data.get('main', {}).get('humidity'),
Â Â Â Â Â Â Â Â 'pressure': weather_data.get('main', {}).get('pressure'),
Â Â Â Â Â Â Â Â 'wind_speed': weather_data.get('wind', {}).get('speed'),
Â Â Â Â Â Â Â Â 'wind_dir': weather_data.get('wind', {}).get('deg'),
Â Â Â Â Â Â Â Â 'weather_source': 'OpenWeatherMap'
Â Â Â Â }
Â Â Â Â for k, v in weather_features.items():
Â Â Â Â Â Â Â Â df_wide[k] = v
Â Â Â 
Â Â Â Â # Add derived features
Â Â Â Â df_wide['aqi_proxy'] = df_wide.get('pm25', 0) * 0.4 + df_wide.get('pm10', 0) * 0.3 +
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â df_wide.get('no2', 0) * 0.2 + df_wide.get('co', 0) * 0.1
Â Â Â Â df_wide['pollution_per_road'] = df_wide.get('pm25', 0) / (df_wide.get('roads_count', 1) + 1)
Â Â Â 
Â Â Â Â # Metadata
Â Â Â Â meta = {
Â Â Â Â Â Â Â Â 'city': city,
Â Â Â Â Â Â Â Â 'latitude': lat,
Â Â Â Â Â Â Â Â 'longitude': lon,
Â Â Â Â Â Â Â Â 'records': len(df_wide),
Â Â Â Â Â Â Â Â 'timestamp': datetime.now(timezone.utc).isoformat(),
Â Â Â Â Â Â Â Â 'source': 'OpenAQ',
Â Â Â Â Â Â Â Â 'unique_stations': df_wide['location_name'].nunique()
Â Â Â Â }
Â Â Â Â return df_wide, meta
def save_datasets(df, filename):
Â Â Â Â if df is None or (isinstance(df, pd.DataFrame) and df.empty):
Â Â Â Â Â Â Â Â st.warning(f"{filename} is empty. Skipping save.")
Â Â Â Â Â Â Â Â return
Â Â Â Â if isinstance(df, dict):
Â Â Â Â Â Â Â Â df = pd.DataFrame([df])
Â Â Â Â df.to_csv(f"{filename}.csv", index=False)
Â Â Â Â df.to_json(f"{filename}.json", orient="records", indent=2)
Â Â Â Â st.success(f"Saved {filename}.csv and {filename}.json")
def consolidate_dataset(df_aq, df_meta, filename):
Â Â Â Â if df_aq is None or df_aq.empty:
Â Â Â Â Â Â Â Â st.warning("AQ dataset empty, skipping consolidation.")
Â Â Â Â Â Â Â Â return
Â Â Â Â for k, v in df_meta.items():
Â Â Â Â Â Â Â Â df_aq[k] = v
Â Â Â Â df_aq.to_csv(f"{filename}.csv", index=False)
Â Â Â Â st.success(f"Consolidated dataset saved as {filename}.csv")
def label_source(row):
Â Â Â Â pm25 = row.get("pm25", 0)
Â Â Â Â roads = row.get("roads_count", 0)
Â Â Â Â industries = row.get("industries_count", 0)
Â Â Â Â farms = row.get("farms_count", 0)
Â Â Â 
Â Â Â Â # More robust labeling logic
Â Â Â Â if pd.notna(pm25) and pm25 > 25 and industries > 0: # Adjusted threshold for pm25
Â Â Â Â Â Â Â Â return "Industrial"
Â Â Â Â elif pd.notna(pm25) and pm25 > 15 and roads > 5: # Adjusted threshold for pm25 and roads
Â Â Â Â Â Â Â Â return "Traffic"
Â Â Â Â elif farms > 0:
Â Â Â Â Â Â Â Â return "Agricultural"
Â Â Â Â else:
Â Â Â Â Â Â Â Â return "Mixed/Other"
# --- Streamlit App ---
st.title("Enviroscan Environmental Data Analysis")
uploaded_file = st.file_uploader("Upload CSV file", type=["csv"])
if uploaded_file:
Â Â Â Â st.info("Processing uploaded file...")
Â Â Â Â city = "Delhi"
Â Â Â Â lat, lon = 28.7041, 77.1025
Â Â Â Â # Build dataset
Â Â Â Â df_aq, meta = build_dataset(city, lat, lon, uploaded_file, OPENWEATHER_KEY)
Â Â Â Â if not df_aq.empty:
Â Â Â Â Â Â Â Â save_datasets(df_aq, "delhi_aq_data")
Â Â Â Â Â Â Â Â save_datasets(meta, "delhi_meta_data")
Â Â Â Â Â Â Â Â consolidate_dataset(df_aq, meta, "delhi_environmental_data")
Â Â Â Â Â Â Â Â st.success("âœ… Dataset processing complete.")
Â Â Â Â Â Â Â Â # --- Data Cleaning ---
Â Â Â Â Â Â Â Â df = pd.read_csv("delhi_environmental_data.csv")
Â Â Â Â Â Â Â Â # --- Preview ---
Â Â Â Â Â Â Â Â st.subheader("ğŸ“Š AQ Dataset Preview")
Â Â Â Â Â Â Â Â st.dataframe(df.head(10))
Â Â Â Â Â Â Â Â # --- Fill missing pollutants ---
Â Â Â Â Â Â Â Â pollutant_cols = [c for c in POLLUTANTS if c in df.columns]
Â Â Â Â Â Â Â Â for col in pollutant_cols:
Â Â Â Â Â Â Â Â Â Â Â Â df[col] = df[col].fillna(df[col].median())
Â Â Â Â Â Â Â Â # --- Fill missing weather ---
Â Â Â Â Â Â Â Â weather_cols = ["temp_c", "humidity", "pressure", "wind_speed", "wind_dir"]
Â Â Â Â Â Â Â Â for col in weather_cols:
Â Â Â Â Â Â Â Â Â Â Â Â if col in df.columns:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â df[col] = df[col].fillna(df[col].mean())
Â Â Â Â Â Â Â Â # --- Ensure OSM features exist ---
Â Â Â Â Â Â Â Â for col in ["roads_count", "industries_count", "farms_count", "dumps_count"]:
Â Â Â Â Â Â Â Â Â Â Â Â if col not in df.columns:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â df[col] = 0
Â Â Â Â Â Â Â Â # --- Create features ---
Â Â Â Â Â Â Â Â if pollutant_cols:
Â Â Â Â Â Â Â Â Â Â Â Â df["aqi_proxy"] = df[pollutant_cols].mean(axis=1)
Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â df["aqi_proxy"] = np.nan
Â Â Â Â Â Â Â Â Â Â Â Â st.warning("âš ï¸ No pollutant columns found, aqi_proxy set to NaN")
Â Â Â Â Â Â Â Â if "pm25" in df.columns and "roads_count" in df.columns:
Â Â Â Â Â Â Â Â Â Â Â Â df["pollution_per_road"] = df["pm25"] / (df["roads_count"] + 1)
Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â df["pollution_per_road"] = np.nan
Â Â Â Â Â Â Â Â Â Â Â Â st.warning("âš ï¸ pm25 or roads_count missing, skipping pollution_per_road")
Â Â Â Â Â Â Â Â df["aqi_category"] = df["aqi_proxy"].apply(
Â Â Â Â Â Â Â Â Â Â Â Â lambda x: (
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "Good" if pd.notna(x) and x <= 50 else
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "Moderate" if pd.notna(x) and x <= 100 else
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "Unhealthy" if pd.notna(x) and x <= 200 else
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "Hazardous"
Â Â Â Â Â Â Â Â Â Â Â Â )
Â Â Â Â Â Â Â Â )
Â Â Â Â Â Â Â Â # --- Assign pollution sources ---
Â Â Â Â Â Â Â Â required_cols = ["pm25", "roads_count", "industries_count", "farms_count"]
Â Â Â Â Â Â Â Â if all(col in df.columns for col in required_cols):
Â Â Â Â Â Â Â Â Â Â Â Â df["pollution_source"] = df.apply(label_source, axis=1)
Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â st.warning("âš ï¸ Required columns for labeling pollution_source are missing. Skipping label assignment.")
Â Â Â Â Â Â Â Â Â Â Â Â df["pollution_source"] = "Unknown"
Â Â Â Â Â Â Â Â # --- Standardize numeric columns ---
Â Â Â Â Â Â Â Â num_cols = ["pm25", "pm10", "no2", "co", "so2", "o3", "roads_count", "industries_count", "farms_count", "dumps_count", "aqi_proxy", "pollution_per_road"] + weather_cols
Â Â Â Â Â Â Â Â num_cols = [col for col in num_cols if col in df.columns]
Â Â Â Â Â Â Â Â scaler = StandardScaler()
Â Â Â Â Â Â Â Â df[num_cols] = scaler.fit_transform(df[num_cols])
Â Â Â Â Â Â Â Â # --- Encode categorical ---
Â Â Â Â Â Â Â Â categorical_cols = ["city", "aqi_category"]
Â Â Â Â Â Â Â Â categorical_cols = [col for col in categorical_cols if col in df.columns]
Â Â Â Â Â Â Â Â if categorical_cols:
Â Â Â Â Â Â Â Â Â Â Â Â df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
Â Â Â Â Â Â Â 
Â Â Â Â Â Â Â Â # Save cleaned dataset
Â Â Â Â Â Â Â Â df.to_csv("cleaned_environmental_data.csv", index=False)
Â Â Â Â Â Â Â Â st.success("ğŸ’¾ Cleaned dataset saved as cleaned_environmental_data.csv")
Â Â Â Â Â Â Â Â # Display preview
Â Â Â Â Â Â Â Â #st.subheader("Preview of Cleaned Dataset")
Â Â Â Â Â Â Â Â #st.dataframe(df.head(10))
Â Â Â Â Â Â Â Â # --- Optional: Model Training ---
Â Â Â Â Â Â Â Â if st.button("Train Models and Predict Pollution Source"):
Â Â Â Â Â Â Â Â Â Â Â Â st.info("Training models...")
Â Â Â Â Â Â Â Â Â Â Â Â X = df.drop(columns=["pollution_source"])
Â Â Â Â Â Â Â Â Â Â Â Â y = df["pollution_source"]
Â Â Â Â Â Â Â Â Â Â Â Â # Ensure consistent samples
Â Â Â Â Â Â Â Â Â Â Â Â st.write(f"X shape: {X.shape}, y shape: {y.shape}")
Â Â Â Â Â Â Â Â Â Â Â Â valid_idx = ~y.isna()
Â Â Â Â Â Â Â Â Â Â Â Â X = X[valid_idx]
Â Â Â Â Â Â Â Â Â Â Â Â y = y[valid_idx]
Â Â Â Â Â Â Â Â Â Â Â Â st.write(f"After cleaning: X shape: {X.shape}, y shape: {y.shape}")
Â Â Â Â Â Â Â Â Â Â Â Â # Check class distribution
Â Â Â Â Â Â Â Â Â Â Â Â st.write("Class distribution before resampling:")
Â Â Â Â Â Â Â Â Â Â Â Â st.write(pd.Series(y).value_counts())
Â Â Â Â Â Â Â Â Â Â Â Â # Visualize class distribution
Â Â Â Â Â Â Â Â Â Â Â Â fig, ax = plt.subplots(figsize=(8, 6))
Â Â Â Â Â Â Â Â Â Â Â Â sns.countplot(x="pollution_source", data=df, ax=ax, palette="viridis")
Â Â Â Â Â Â Â Â Â Â Â Â ax.set_title("Pollution Source Distribution")
Â Â Â Â Â Â Â Â Â Â Â Â ax.set_xlabel("Pollution Source")
Â Â Â Â Â Â Â Â Â Â Â Â ax.set_ylabel("Count")
Â Â Â Â Â Â Â Â Â Â Â Â plt.xticks(rotation=45)
Â Â Â Â Â Â Â Â Â Â Â Â st.pyplot(fig)
Â Â Â Â Â Â Â Â Â Â Â Â # Keep only numeric features
Â Â Â Â Â Â Â Â Â Â Â Â X = X.select_dtypes(include=[np.number])
Â Â Â Â Â Â Â Â Â Â Â Â numeric_columns = X.columns.tolist() # Save columns before transforming
Â Â Â Â Â Â Â Â Â Â Â Â # Impute and scale
Â Â Â Â Â Â Â Â Â Â Â Â imputer = SimpleImputer(strategy="median")
Â Â Â Â Â Â Â Â Â Â Â Â X = imputer.fit_transform(X)
Â Â Â Â Â Â Â Â Â Â Â Â scaler = StandardScaler()
Â Â Â Â Â Â Â Â Â Â Â Â X = scaler.fit_transform(X)
Â Â Â Â Â Â Â Â Â Â Â Â # Use simpler models
Â Â Â Â Â Â Â Â Â Â Â Â models = {
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "Logistic Regression": LogisticRegression(max_iter=1000, C=0.1, random_state=42)
Â Â Â Â Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â Â Â Â Â Â Â # Use cross-validation for small datasets
Â Â Â Â Â Â Â Â Â Â Â Â if X.shape[0] < 50: # Arbitrary threshold for small datasets
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.warning("Small dataset detected. Using cross-validation instead of train-test split.")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â for name, model in models.items():
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â scores = cross_validate(model, X, y, cv=KFold(n_splits=5, shuffle=True, random_state=42), scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'], return_train_score=False)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.write(f"{name} Cross-Validation Results:")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.write(f"Accuracy: {scores['test_accuracy'].mean():.2f} Â± {scores['test_accuracy'].std():.2f}")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.write(f"Precision: {scores['test_precision_weighted'].mean():.2f} Â± {scores['test_precision_weighted'].std():.2f}")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.write(f"Recall: {scores['test_recall_weighted'].mean():.2f} Â± {scores['test_recall_weighted'].std():.2f}")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.write(f"F1: {scores['test_f1_weighted'].mean():.2f} Â± {scores['test_f1_weighted'].std():.2f}")
Â Â Â Â Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â # Split data
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â from imblearn.over_sampling import SMOTE
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â # Balance training data with SMOTE
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â if len(y_train.value_counts()) > 1 and min(y_train.value_counts()) > 1:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â smote = SMOTE(random_state=42)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â X_train, y_train = smote.fit_resample(X_train, y_train)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.write("Class distribution after SMOTE:")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.write(pd.Series(y_train).value_counts())
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.warning("Not enough samples for SMOTE. Proceeding with original training data.")
Â Â Â 
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â # Impute and scale
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â X_train = imputer.fit_transform(X_train)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â X_test = imputer.transform(X_test)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â X_train = scaler.fit_transform(X_train)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â X_test = scaler.transform(X_test)
Â Â Â 
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â performance = {}
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â for name, model in models.items():
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.write(f"Training {name}...")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â model.fit(X_train, y_train)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â y_pred = model.predict(X_test)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â acc = accuracy_score(y_test, y_pred)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â prec = precision_score(y_test, y_pred, average="weighted", zero_division=0)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â rec = recall_score(y_test, y_pred, average="weighted", zero_division=0)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â f1 = f1_score(y_test, y_pred, average="weighted", zero_division=0)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â performance[name] = {"Accuracy": acc, "Precision": prec, "Recall": rec, "F1": f1}
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.write(f"Test results for {name}:")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.text(classification_report(y_test, y_pred, zero_division=0))
Â Â Â Â Â Â Â 
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â # Confusion matrix
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â cm = confusion_matrix(y_test, y_pred, labels=model.classes_)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â fig, ax = plt.subplots(figsize=(6, 4))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=model.classes_, yticklabels=model.classes_, ax=ax)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ax.set_title(f"Confusion Matrix - {name}")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ax.set_xlabel("Predicted")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ax.set_ylabel("Actual")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.pyplot(fig)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â # Save best model
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â best_model_name = max(performance, key=lambda k: performance[k]["F1"])
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â best_model = models[best_model_name]
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â joblib.dump(best_model, "pollution_source_model.pkl")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.success(f"ğŸ’¾ Best model saved as pollution_source_model.pkl")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â # Save predictions
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â X_test_orig = pd.DataFrame(X_test, columns=numeric_columns) # Use saved columns
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â X_test_orig["actual_source"] = y_test.reset_index(drop=True)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â X_test_orig["predicted_source"] = y_pred
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â X_test_orig.to_csv("final_predictions.csv", index=False)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.success("ğŸ’¾ Final predictions saved as final_predictions.csv")
