# -*- coding: utf-8 -*-
"""Enviroscan7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19NAfuYkYngHmsos0L2t5jgY62lBVSEJz
"""


import requests
import pandas as pd
import numpy as np
import osmnx as ox
from datetime import datetime, timezone
from sklearn.preprocessing import StandardScaler
from google.colab import files

def option1_upload_and_save():
    uploaded = files.upload()
    for fn in uploaded.keys():
        print(f"üìÇ Uploaded file: {fn}")
        df = pd.read_csv(fn, skiprows=2, on_bad_lines="skip", engine="python")
        save_datasets(df, "uploaded_dataset")

# Weather API Key
openweather_key = "f931ecc3a4864ae98a35630e7a9f5bc2"

# List of pollutants to keep track of
POLLUTANTS = ["pm25", "pm10", "no2", "co", "so2", "o3"]

def load_aq_csv(path):
    df = pd.read_csv(path, skiprows=2, on_bad_lines="skip", engine="python")
    df = df.loc[:, ~df.columns.str.contains("^Unnamed")]
    return df

def get_weather(lat, lon, api_key):
    url = "http://api.openweathermap.org/data/2.5/weather"
    resp = requests.get(url, params={"lat": lat, "lon": lon, "appid": api_key, "units": "metric"})
    return resp.json() if resp.status_code == 200 else {}

def extract_osm_features(lat, lon, radius=100):
    features = {}
    point = (lat, lon)

    try:
        roads = ox.features_from_point(point, tags={"highway": True}, dist=radius)
        features["roads_count"] = len(roads)
    except:
        features["roads_count"] = 0

    try:
        industries = ox.features_from_point(point, tags={"landuse": ["industrial", "commercial"]}, dist=radius)
        features["industries_count"] = len(industries)
    except:
        features["industries_count"] = 0

    try:
        farms = ox.features_from_point(point, tags={"landuse": ["farmland", "farm", "agricultural"]}, dist=radius)
        features["farms_count"] = len(farms)
    except:
        features["farms_count"] = 0

    try:
        dumps = ox.features_from_point(point, tags={"landuse": ["landfill", "waste", "dump"]}, dist=radius)
        features["dumps_count"] = len(dumps)
    except:
        features["dumps_count"] = 0

    return features

def build_dataset(city, lat, lon, aq_csv_path, openweather_key):
    print(f"Collecting AQ data for {city} from CSV...")

    # --- Load AQ CSV safely ---
    try:
        df_aq = pd.read_csv(
            aq_csv_path,
            sep=",",
            engine="python",
            skiprows=2,
            on_bad_lines="skip"
        )
        df_aq = df_aq.loc[:, ~df_aq.columns.str.contains("^Unnamed")]
        df_aq["source"] = "OpenAQ"
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to load AQ CSV: {e}")
        return pd.DataFrame(), {}

    # --- Horizontal pivot preview ---
    try:
        df_preview = df_aq[['location_name', 'parameter', 'value']]
        df_horizontal = df_preview.pivot_table(
            index='location_name',
            columns='parameter',
            values='value',
            aggfunc='first'
        ).reset_index()

        print("\nüìä AQ Dataset Preview (horizontal format):")
        print(df_horizontal.head(10))
    except Exception as e:
        print(f"‚ö†Ô∏è Could not pivot for horizontal view: {e}")
        print(df_aq.head())

    # --- Weather features ---
    try:
        weather_data = get_weather(lat, lon, openweather_key)
        weather_features = {
            "temp_c": weather_data.get("main", {}).get("temp", None),
            "humidity": weather_data.get("main", {}).get("humidity", None),
            "pressure": weather_data.get("main", {}).get("pressure", None),
            "wind_speed": weather_data.get("wind", {}).get("speed", None),
            "wind_dir": weather_data.get("wind", {}).get("deg", None),
            "weather_source": "OpenWeatherMap"
        }
    except:
        weather_features = {"temp_c": None, "humidity": None, "pressure": None, "wind_speed": None, "wind_dir": None}

    # --- OSM features ---
    try:
        G = ox.graph_from_point((lat, lon), dist=2000, network_type="drive")
        gdf = ox.geocode_to_gdf(city)
        gdf_utm = gdf.to_crs(epsg=32643)
        area_km2 = gdf_utm.geometry.area.iloc[0] / 1e6
        road_count = len(G.edges())
        osm_features = extract_osm_features(lat, lon, radius=2000)
        osm_features["osm_area_km2"] = area_km2
        osm_features["road_count"] = road_count
        osm_features["osm_source"] = "OpenStreetMap/OSMnx"
    except Exception as e:
        print(f"‚ö†Ô∏è OSM data fetch failed: {e}")
        osm_features = {"osm_area_km2": None, "road_count": None}

    # --- Metadata dictionary ---
    meta = {
        "city": city,
        "latitude": lat,
        "longitude": lon,
        "records": len(df_aq),
        "timestamp": datetime.now(timezone.utc).isoformat(),
    }
    meta.update(weather_features)
    meta.update(osm_features)

    print("\nüìä Metadata + Weather + OSM:\n", meta)

    return df_aq, meta

def save_datasets(df, filename):
    if df is None or (isinstance(df, pd.DataFrame) and df.empty):
        print(f"‚ö†Ô∏è {filename} is empty. Skipping save.")
        return
    if isinstance(df, dict):
        df = pd.DataFrame([df])
    df.to_csv(f"{filename}.csv", index=False)
    df.to_json(f"{filename}.json", orient="records", indent=2)
    print(f"üíæ Saved {filename}.csv and {filename}.json")

def consolidate_dataset(df_aq, df_meta, filename):
    if df_aq is None or df_aq.empty:
        print("‚ö†Ô∏è AQ dataset empty, skipping consolidation.")
        return
    for k, v in df_meta.items():
        df_aq[k] = v
    df_aq.to_csv(f"{filename}.csv", index=False)
    print(f"üíæ Consolidated dataset saved as {filename}.csv")

# === Main Execution ===
city = "Delhi"
lat, lon = 28.7041, 77.1025

uploaded = files.upload()
aq_csv_path = list(uploaded.keys())[0]
print(f"üìÇ Using uploaded file: {aq_csv_path}")

df_aq, df_meta = build_dataset(city, lat, lon, aq_csv_path, openweather_key)

save_datasets(df_aq, "delhi_aq_data")
save_datasets(df_meta, "delhi_meta_data")
consolidate_dataset(df_aq, df_meta, "delhi_environmental_data")

print("\n=== Data Cleaning and Feature Engineering ===")
df = pd.read_csv("delhi_environmental_data.csv")
print(f"Dataset Loaded: {df.shape[0]} rows, {df.shape[1]} columns")

# Ensure pollutants are in columns (pivot if needed)
if "parameter" in df.columns and "value" in df.columns:
    print("Pivoting dataset so pollutants become columns...")
    df = df.pivot_table(
        index=["location_name", "city", "latitude", "longitude", "timestamp"],
        columns="parameter",
        values="value",
        aggfunc="first"
    ).reset_index()
    df.columns.name = None  # remove pivot column name

# Ensure OSM features exist
osm_features = ["roads_count", "industries_count", "farms_count", "dumps_count"]
for col in osm_features:
    if col not in df.columns:
        df[col] = 0  # fallback default

# Missing values
df = df.replace({None: np.nan})
for col in POLLUTANTS:
    if col in df.columns:
        df[col] = df[col].fillna(df[col].median())

weather_cols = ["temperature", "humidity", "pressure", "wind_speed", "wind_dir"]
for col in weather_cols:
    if col in df.columns:
        df[col] = df[col].fillna(df[col].mean())

# Duplicates
before = df.shape[0]
df = df.drop_duplicates()
after = df.shape[0]
print(f"Removed {before - after} duplicate rows")

# Drop irrelevant
drop_cols = ["data_source", "openaq_api_version", "openweathermap_api_version", "osmnx_version"]
df = df.drop(columns=drop_cols, errors="ignore")

# Make sure pollutants exist
for col in ["pm25","pm10","no2","o3"]:
    if col not in df.columns:
        df[col] = np.nan

# Make sure OSM features exist
for col in ["roads_count", "industries_count", "farms_count", "dumps_count"]:
    if col not in df.columns:
        df[col] = 0

# Create features
df["aqi_proxy"] = df[["pm25","pm10","no2","o3"]].mean(axis=1)
df["pollution_per_road"] = df["pm25"] / (df["roads_count"] + 1)

def categorize_aqi(val):
    if val <= 50: return "Good"
    elif val <= 100: return "Moderate"
    elif val <= 200: return "Unhealthy"
    else: return "Hazardous"

df["aqi_category"] = df["aqi_proxy"].apply(categorize_aqi)

# Adjust weather column naming
weather_cols = ["temp_c", "humidity", "pressure", "wind_speed", "wind_dir"]

# Build final list of numeric columns
num_cols = ["pm25","pm10","no2","co","so2","o3",
            "roads_count","industries_count","farms_count","dumps_count",
            "aqi_proxy","pollution_per_road"] + weather_cols

# Keep only existing columns
num_cols = [col for col in num_cols if col in df.columns]

# Standardize numeric
scaler = StandardScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])
print(f"Standardized {len(num_cols)} numerical features")

categorical_cols = ["city", "aqi_category", "aqi_simple"]

# Keep only those that exist in df
categorical_cols = [col for col in categorical_cols if col in df.columns]

if categorical_cols:
    df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
    print(f"Encoded categorical features: {categorical_cols}")
else:
    print("‚ö†Ô∏è No categorical columns found to encode")

# Save cleaned dataset
df.to_csv("cleaned_environmental_data.csv", index=False)
print("üíæ Cleaned dataset saved as cleaned_environmental_data.csv")

df_clean = pd.read_csv("cleaned_environmental_data.csv")
print(df_clean.shape)   # number of rows and columns
df_clean.head(10)       # first 10 rows

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier

df = pd.read_csv("cleaned_environmental_data.csv")

import numpy as np

dfs = []
for i in range(100):  # simulate ~100 rows
    temp = df.copy()
    temp["pm25"] += np.random.normal(0, 0.1, size=len(df))
    temp["pm10"] += np.random.normal(0, 0.1, size=len(df))
    temp["roads_count"] += np.random.randint(0,2, size=len(df))
    temp["industries_count"] += np.random.randint(0,2, size=len(df))
    temp["farms_count"] += np.random.randint(0,2, size=len(df))
    dfs.append(temp)

df = pd.concat(dfs, ignore_index=True)
print("‚úÖ Expanded dataset shape:", df.shape)

# Optionally save expanded dataset
df.to_csv("cleaned_environmental_data_expanded.csv", index=False)

def label_source(row):
    # Use 0 as the threshold since scaled data has mean=0
    if row["pm25"] > 0 and row.get("industries_count", 0) > 0:
        return "Industrial"
    elif row["pm25"] > 0 and row.get("roads_count", 0) > 0:
        return "Traffic"
    elif row.get("farms_count", 0) > 0:
        return "Agricultural"
    else:
        return "Mixed/Other"

df["pollution_source"] = df.apply(label_source, axis=1)

print("‚úÖ Source labels assigned using rule-based simulation")
print(df["pollution_source"].value_counts())

# Save labeled dataset
df.to_csv("source_labeled_data.csv", index=False)
print("üíæ Source-labeled dataset saved as source_labeled_data.csv")

print(df["pollution_source"].value_counts())

print("\n=== Model Training and Source Prediction ===")

from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

# Define features and target
X = df.drop(columns=["pollution_source"])
y = df["pollution_source"]

# Keep only numeric features (drop text like city/location_name)
X = X.select_dtypes(include=[np.number])

# Train/validation/test split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# Keep original X_test for saving predictions later
# ----------------------------
X_test_orig = X_test.copy()  # <--- ADD THIS HERE

print(f"Train size: {X_train.shape}, Validation size: {X_val.shape}, Test size: {X_test.shape}")

# Balance dataset (optional, if classes are imbalanced)
df_train = pd.concat([X_train, y_train], axis=1)
majority_class = df_train["pollution_source"].value_counts().idxmax()
dfs = []
for label in df_train["pollution_source"].unique():
    subset = df_train[df_train["pollution_source"] == label]
    if label != majority_class:
        subset = resample(subset, replace=True, n_samples=df_train[df_train["pollution_source"] == majority_class].shape[0], random_state=42)
    dfs.append(subset)
df_train_balanced = pd.concat(dfs)

X_train = df_train_balanced.drop(columns=["pollution_source"])
y_train = df_train_balanced["pollution_source"]

print("‚úÖ Applied class balancing")

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

imputer = SimpleImputer(strategy="median")
X_train = imputer.fit_transform(X_train)
X_val   = imputer.transform(X_val)
X_test  = imputer.transform(X_test)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val   = scaler.transform(X_val)
X_test  = scaler.transform(X_test)

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=200, random_state=42),
    "Neural Network": MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)
}

performance = {}

for name, model in models.items():
    print(f"\nüîπ Training {name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)

    acc = accuracy_score(y_val, y_pred)
    prec = precision_score(y_val, y_pred, average="weighted", zero_division=0)
    rec = recall_score(y_val, y_pred, average="weighted", zero_division=0)
    f1 = f1_score(y_val, y_pred, average="weighted", zero_division=0)

    performance[name] = {"Accuracy": acc, "Precision": prec, "Recall": rec, "F1": f1}

    print(f"Validation Results for {name}:")
    print(classification_report(y_val, y_pred, zero_division=0))

    # Confusion matrix plot
    cm = confusion_matrix(y_val, y_pred, labels=model.classes_)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=model.classes_, yticklabels=model.classes_)
    plt.title(f"Confusion Matrix - {name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

best_model_name = max(performance, key=lambda k: performance[k]["F1"])
best_model = models[best_model_name]

print(f"\nüèÜ Best Model Selected: {best_model_name}")

# Evaluate on test set
y_test_pred = best_model.predict(X_test)
print("\nFinal Test Performance:")
print(classification_report(y_test, y_test_pred, zero_division=0))

# Save trained model
joblib.dump(best_model, "pollution_source_model.pkl")
print("üíæ Best model saved as pollution_source_model.pkl")

# Use X_test_orig to preserve column names
X_test_orig["actual_source"] = y_test.reset_index(drop=True)
X_test_orig["predicted_source"] = y_test_pred
X_test_orig.to_csv("final_predictions.csv", index=False)
print("üíæ Final predictions saved as final_predictions.csv")
